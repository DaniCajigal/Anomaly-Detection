# -*- coding: utf-8 -*-
"""anomaly-detection-training-autoencoder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hzYrgB0zLPUcjXeaUGG07nK3syZNkAFT
"""

from zipfile import ZipFile
file_name = "dataset.zip"

with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print('Done')

!python -m pip install matplotlib
!python -m pip install pandas
!python -m pip install scikit-learn
!python -m pip install tensorflow==2.1.0
!python -m pip install keras
!python -m pip install numpy==1.18.1
!python -m pip install scipy==1.8.1

from os import listdir
from os.path import join
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
from scipy import stats
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, regularizers
from sklearn.metrics import confusion_matrix
import seaborn as sn

# Print versions
import sys
import keras
import scipy
print('Python', sys.version)
print('Numpy', np.__version__)
print('TensorFlow', tf.__version__)
print('Keras', keras.__version__)
print('Scipy', scipy.__version__)

# Settings
dataset_path = 'dataset/'  # Directory where raw accelerometer data is stored
normal_op_list = ['normal']
anomaly_op_list = ['colectiva']
mixture_op_list = ['puntual']
val_ratio = 0.2             # Percentage of samples that should be held for validation set
test_ratio = 0.2            # Percentage of samples that should be held for test set
raw_scale = 1               # Multiply raw values to fit into integers
sensor_sample_rate = 200    # Hz
desired_sample_rate = 50    # Hz
sample_time = 0.64           # Time (sec) length of each sample
samples_per_file = 128      # Expected number of measurements in each file (truncate to this)
max_measurements = int(sample_time * sensor_sample_rate)
downsample_factor = int(samples_per_file / desired_sample_rate)
win_len = int(max_measurements / downsample_factor)

keras_model_name = 'models/autoencoder'           # Will be given .h5 suffix
sample_file_name = 'test_samples/normal_anomaly_samples'  # Will be given .npz suffix
rep_dataset_name = 'test_samples/normal_anomaly_test_set' # Will be given .npz suffix

print('Max measurements per file:', max_measurements)
print('Downsample factor:', downsample_factor)
print('Window length:', win_len)

# Create list of filenames
def createFilenameList(op_list):

    # Extract paths and filenames in each directory
    op_filenames = []
    num_samples = 0
    for index, target in enumerate(op_list):
        samples_in_dir = listdir(join(dataset_path, target))
        samples_in_dir = [join(dataset_path, target, sample) for sample in samples_in_dir]
        op_filenames.append(samples_in_dir)

    # Flatten list
    return [item for sublist in op_filenames for item in sublist]

# Create normal and anomaly filename lists
normal_op_filenames = createFilenameList(normal_op_list)
anomaly_op_filenames = createFilenameList(anomaly_op_list)
print('Number of normal samples:', len(normal_op_filenames))
print('Number of anomaly samples:', len(anomaly_op_filenames))

# Shuffle lists
random.shuffle(normal_op_filenames)
random.shuffle(anomaly_op_filenames)

# Calculate validation and test set sizes
val_set_size = int(len(normal_op_filenames) * val_ratio)
test_set_size = int(len(normal_op_filenames) * test_ratio)

# Break dataset apart into train, validation, and test sets
num_samples = len(normal_op_filenames)
filenames_val_normal = normal_op_filenames[:val_set_size]
filenames_test_normal = normal_op_filenames[val_set_size:(val_set_size + test_set_size)]
filenames_train = normal_op_filenames[(val_set_size + test_set_size):]

# Print out number of samples in each set
print('Number of training samples:', len(filenames_train))
print('Number of validation samples:', len(filenames_val_normal))
print('Number of test samples:', len(filenames_test_normal))

# Check that our splits add up correctly
assert(len(filenames_train) + len(filenames_val_normal) + len(filenames_test_normal)) == num_samples

def extract_features(sample, max_measurements=0, scale=1):
    features = []

    # Truncate sample
    if max_measurements == 0:
        max_measurements = sample.shape[0]
    sample = sample[0:max_measurements]

    # Scale sample
    sample = scale * sample

    # Median absolute deviation (MAD)
    features.append(stats.median_abs_deviation(sample))

    return np.array(features).flatten()

# Test with 1 sample
sample = np.genfromtxt(filenames_test_normal[0], delimiter=',')
features = extract_features(sample, max_measurements, scale=raw_scale)
print(features.shape)
print(features)
plt.plot(features)

# Function: loop through filenames, creating feature sets
def create_feature_set(filenames):
    x_out = []
    for file in filenames:
        sample = np.genfromtxt(file, delimiter=',')
        features = extract_features(sample, max_measurements, raw_scale)
        x_out.append(features)

    return np.array(x_out)

# Create training, validation, and test sets
x_train = create_feature_set(filenames_train)
print('Extracted features from training set. Shape:', x_train.shape)
x_val = create_feature_set(filenames_val_normal)
print('Extracted features from validation set. Shape:', x_val.shape)
x_test = create_feature_set(filenames_test_normal)
print('Extracted features from test set. Shape:', x_test.shape)

# Get input shape for 1 sample
sample_shape = x_train.shape[1:]
print(sample_shape)

# Build model
# Based on: https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd
encoding_dim = 2       # Number of nodes in first layer
model = models.Sequential([
    layers.InputLayer(input_shape=sample_shape),
    layers.Dense(encoding_dim, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(*sample_shape, activation='relu')
])

# Display model
model.summary()

# Add training parameters to model
model.compile(optimizer='adam',
             loss='mse')

# Train model (note Y labels are same as inputs, X)
history = model.fit(x_train,
                   x_train,
                   epochs=50,
                   batch_size=100,
                   validation_data=(x_val, x_val),
                   verbose=1)

# Plot results
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(loss) + 1)

plt.figure()

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

# Calculate MSE from validation set
predictions = model.predict(x_val)
normal_mse = np.mean(np.power(x_val - predictions, 2), axis=1)
print('Average MSE for normal validation set:', np.average(normal_mse))
print('Standard deviation of MSE for normal validation set:', np.std(normal_mse))
print('Recommended threshold (3x std dev + avg):', (3*np.std(normal_mse)) + np.average(normal_mse))
fig, ax = plt.subplots(1,1)
ax.hist(normal_mse, bins=20, label='normal', color='blue', alpha=0.7)

# Extract features from anomaly test set (truncate to length of X test set)
anomaly_ops_trunc = anomaly_op_filenames[0:len(normal_mse)]
anomaly_features = create_feature_set(anomaly_ops_trunc)
print('Extracted features from anomaly set. Shape:', anomaly_features.shape)

# Calculate MSE from anomaly set
predictions = model.predict(anomaly_features)
anomaly_mse = np.mean(np.power(anomaly_features - predictions, 2), axis=1)
print('Average MSE for for anomaly test set:', np.average(anomaly_mse))

# Plot histograms of normal validation vs. anomaly sets (MSEs)
fig, ax = plt.subplots(1,1)
plt.xscale("log")
ax.hist(normal_mse, bins=20, label='normal', color='blue', alpha=0.7)
ax.hist(anomaly_mse, bins=20, label='anomaly', color='red', alpha=0.7)

# Look at separation using test set
predictions = model.predict(x_test)
normal_mse = np.mean(np.power(x_test - predictions, 2), axis=1)
print('Average MSE for normal test set:', np.average(normal_mse))

# Plot histograms of normal test vs. anomaly sets (MSEs)
fig, ax = plt.subplots(1,1)
plt.xscale("log")
ax.hist(normal_mse, bins=20, label='normal', color='blue', alpha=0.7)
ax.hist(anomaly_mse, bins=20, label='anomaly', color='red', alpha=0.7)

# If we're happy with the performance, save the model
model.save(keras_model_name + '.h5')

# Save a normal and anomaly sample for trying out on the MCU
normal_sample = np.genfromtxt(filenames_test_normal[0], delimiter=',')
anomaly_sample = np.genfromtxt(anomaly_op_filenames[0], delimiter=',')
np.savez(sample_file_name + '.npz', normal_sample=normal_sample, anomaly_sample=anomaly_sample)

# Save the test dataset for use as a representative dataset
np.savez(rep_dataset_name + '.npz', x_test=x_test)

# Create a classifier (0 = normal, 1 = anomaly)
def detect_anomaly(x, model, threshold=0):
    input_tensor = x_test[0].reshape(1, -1)
    pred = model.predict(input_tensor)
    mse = np.mean(np.power(x - pred, 2), axis=1)
    if mse > threshold:
        return 1
    else:
        return 0

# Choose a threshold
anomaly_threshold = 0.005

# Perform classification on test set
pred_test = [detect_anomaly(x, model, anomaly_threshold) for x in x_test]
print(pred_test)

# Perform classification on anomaly set
pred_anomaly = [detect_anomaly(x, model, anomaly_threshold) for x in anomaly_features]
print(pred_anomaly)

# Combine predictions into one long list and create a label list
pred = np.array(pred_test + pred_anomaly)
labels = ([0] * len(pred_test)) + ([1] * len(pred_anomaly))

# Create confusion matrix
cm = confusion_matrix(labels, pred)
print(cm)

# Make confusion matrix pretty
df_cm = pd.DataFrame(cm, index=['normal', 'anomaly'], columns=['normal', 'anomaly'])
plt.figure()
sn.heatmap(df_cm, annot=True)
plt.title('Confusion Matrix')
plt.xlabel('Prediction')
plt.ylabel('Actual')

mixture_op_filenames = createFilenameList(mixture_op_list)
print('Number of mixture samples:', len(mixture_op_filenames))
mixture_ops_trunc = mixture_op_filenames[0:len(normal_mse)]
mixture_features = create_feature_set(mixture_ops_trunc)
print('Extracted features from anomaly set. Shape:', mixture_features.shape)
# Perform classification on mixture set
pred_anomaly = [detect_anomaly(x, model, anomaly_threshold) for x in mixture_features]
print(pred_anomaly)

from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report

val_anomaly_size = int(len(filenames_val_normal) * 0.5)  # Por ejemplo, 50% de las muestras de validación serán anómalas
test_anomaly_size = int(len(filenames_test_normal) * 0.5)

# Crear conjuntos de validación y prueba realistas
filenames_val = filenames_val_normal + anomaly_op_filenames[:val_anomaly_size]
filenames_test = filenames_test_normal + anomaly_op_filenames[val_anomaly_size:val_anomaly_size + test_anomaly_size]

# Mezclar los conjuntos
random.shuffle(filenames_val)
random.shuffle(filenames_test)

print('Number of validation samples (mixed):', len(filenames_val))
print('Number of test samples (mixed):', len(filenames_test))

x_val = create_feature_set(filenames_val)
print('Extracted features from validation set. Shape:', x_val.shape)
x_test = create_feature_set(filenames_test)
print('Extracted features from test set. Shape:', x_test.shape)

# Predecir anomalías en el conjunto de validación
val_predictions = model.predict(x_val)
val_anomaly_scores = np.mean(np.square(x_val - val_predictions), axis=1)

# Predecir anomalías en el conjunto de prueba
test_predictions = model.predict(x_test)
test_anomaly_scores = np.mean(np.square(x_test - test_predictions), axis=1)

# Establecer un umbral para clasificar las anomalías basado en los datos de validación
threshold = 0.001

# Convertir las puntuaciones de anomalía a etiquetas (0 = normal, 1 = anomalía)
val_predictions = [1 if score > threshold else 0 for score in val_anomaly_scores]
test_predictions = [1 if score > threshold else 0 for score in test_anomaly_scores]

# Generar etiquetas para la matriz de confusión (0 = normal, 1 = anomalía)
val_labels = [0 if 'normal' in f else 1 for f in filenames_val]
test_labels = [0 if 'normal' in f else 1 for f in filenames_test]

# Crear y mostrar la matriz de confusión
cm = confusion_matrix(test_labels, test_predictions)
print(cm)

# Mostrar la matriz de confusión de manera más visual
df_cm = pd.DataFrame(cm, index=['normal', 'anomaly'], columns=['normal', 'anomaly'])
plt.figure()
sn.heatmap(df_cm, annot=True)
plt.title('Confusion Matrix')
plt.xlabel('Prediction')
plt.ylabel('Actual')
plt.show()

# Calcular métricas adicionales
precision = precision_score(test_labels, test_predictions)
recall = recall_score(test_labels, test_predictions)
f1 = f1_score(test_labels, test_predictions)

# Imprimir las métricas
print('Precision:', precision)
print('Recall:', recall)
print('F1 Score:', f1)

# Generar e imprimir el informe de clasificación
classification_rep = classification_report(test_labels, test_predictions, target_names=['normal', 'anomaly'])
print(classification_rep)